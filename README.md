# LLM Bias Detection in Data Narratives  
### Syracuse University â€“ IST Research Task 08  
### Author: Sathvik Mateti

---

## ğŸ“Œ Project Overview
This project analyzes **bias in Large Language Models (LLMs)** when interpreting the *same dataset* under different prompt conditions. Using the 2025 Syracuse Womenâ€™s Lacrosse statistics, the experiment evaluates whether models produce biased narratives based on:

1. **Framing Bias (H1)** â€“ Positive vs. negative prompt framing  
2. **Identity Bias (H2)** â€“ Named player vs. anonymized player  
3. **Confirmation Bias (H3)** â€“ Neutral prompt vs. â€œunderperformanceâ€ assumption  

Three LLMs were tested:

- **ChatGPT**
- **Claude**
- **Gemini**

Each model was queried across **three runs per condition**, generating a total of **54 responses**.

All results, prompts, analysis code, and outputs are included in this repository.

---

## ğŸ“‚ Repository Structure

```plaintext
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ prompts.csv
â”‚   â””â”€â”€ prompts.json
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ Run1_chatgpt_responses.json
â”‚   â”œâ”€â”€ Run1_claude_responses.json
â”‚   â”œâ”€â”€ Run1_gemini_responses.json
â”‚   â”œâ”€â”€ Run2_chatgpt_responses.json
â”‚   â””â”€â”€ ... (all raw model outputs)
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ sentiment_by_condition.csv
â”‚   â”œâ”€â”€ sentiment_by_condition_model.csv
â”‚   â”œâ”€â”€ entity_mentions.csv
â”‚   â”œâ”€â”€ recommendations_by_condition.csv
â”‚   â”œâ”€â”€ stat_ttests.csv
â”‚   â”œâ”€â”€ stat_chi_square.csv
â”‚   â”œâ”€â”€ validation_flags.csv
â”‚   â””â”€â”€ fabrication_rates_by_condition.csv
â”‚
â”œâ”€â”€ experiment_design.py
â”œâ”€â”€ run_experiment.py
â”œâ”€â”€ analyze_bias.py
â”œâ”€â”€ statistical_tests.py
â”œâ”€â”€ validate_claims.py
â”œâ”€â”€ visualize_bias.py
â”‚
â”œâ”€â”€ REPORT.md
â””â”€â”€ README.md
